\documentclass[10pt,reqno]{amsart}
\usepackage{articlesetup}
\usepackage{float}

\bibliography{bibliography}

\author{Harvey J. Stein}
\email{hjstein@bloomberg.net}

\date{\today}

\begin{document}

\title{COVID-19 data collection: Garbage In, Garbage Out}

\begin{abstract}
  I analyze anomalies in the data collected by the major COVID-19 data
  repositories.  These anomalies are due to how the data is
  aggregated.  I find that the method used tends to understate the
  rise in the infection rate, overstate the fall after the peak, and
  generate spurious peaks and drops.  As a result, relying on this
  data can lead to delaying taking action during the rise up to a peak
  and delaying normalization after the fall.

  I make recommendations for changing the aggregation methodology to
  correct for these problems.  Moreover, rather than aggregating the
  data at all, making the raw data available would both improve the
  situation as well as make more accurate and detailed analysis
  possible.
\end{abstract}

\maketitle
\tableofcontents

\section{Introduction}
\label{sec:intro}
COVID-19 data is collected, aggregated, and published by a variety of
agencies.  The data itself comes from a variety of sources, such as
hospitals and testing facilities.  The sources generally do not report
cases on a daily basis due to various delays involved.  Some sources
might report weekly, and others may report monthly.  As a result, the
reported totals for a given date will be understated until reports
from all sources arrive at the aggregating agency, which can take
as much as a month's time.

My 4/21 analysis of the NYC COVID-19 data
(\href{https://hjstein.blogspot.com/2020/04/covid-19-nyc-stats-not-what-they-seem.html}{COVID-19 NYC Stats -- Not What They Seem}),
shows a graph demonstrating that this missing
data can have a major impact on the reported incident
counts.\nocite{nyc2020data,Stein2020nycdata,Stein2020owiddata,owid2020data}
Figure~\ref{fig:daily} contains a similar graph for NYC's data around
the first wave's peak.\nocite{Stein2020Seem}\nocite{Stein2020Ray}

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{../Notebooks/Wave1Peak.png}
  \caption[NYC COVID-19 cases per day, each daily report, first
  peak]{NYC COVID-19 cases per day from each daily report around the
    peak of the first wave, from the NYC COVID-19 data github
    repository.  It can take a month before the totals are complete.
    Initial reports sometimes include as little as one fifth of the
    total cases.}
  \label{fig:daily}
\end{figure}

Comparing the counts for a given date as they are updated over time
shows that it can take a month or more before all of the data for
a given date finally arrives.  Moreover, the final count for a given date
can be five times larger than the initial count.
Figure~\ref{fig:smoothDaily} has the 7 day rolling averages for the same
time period.  The 7 day averaging adjusts for the weekend dip that the
data has, and reduces the discrepancy between the first and final
counts, but still shows substantial lags in data collection.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{../Notebooks/Wave1SmoothedPeak.png}
  \caption{7-day rolling average of NYC COVID-19 cases per day from
    each daily report, from the NYC COVID-19 data github repository.
    The missing data has less impact on the rolling average, but their
  effect is still substantial, especially near the peak.}
  \label{fig:smoothDaily}
\end{figure}

After seeing this analysis, Jon Asmundsson, editor of {\it Bloomberg
  Markets} asked me if this holds for other regions. \cite{Asmundsson2020Dates}
%% \begin{quotation}
%%   Your analysis is really interesting.  Have you looked at other states/cities?
%% \end{quotation}

This led me to take a look, the results of which I originally wrote up
in
\href{https://hjstein.blogspot.com/2020/05/covid-19-data-collection-garbage-in_33.html}{COVID-19
  data collection: Garbage In, Garbage Out}.\nocite{Stein2020Garbage}  I revisit the analysis
here, adding an analysis of the Omicron data.

\section{Garbage in}
My first stop was the
\href{https://github.com/owid/covid-19-data}{Our World in Data github
  repository}\nocite{owid2020data}.  I
\href{https://github.com/hjstein/covid-19-data}{forked the
  repository}, imported my analysis code, extracted the historical
reports and graphed the histories.  The results are in Figure~\ref{fig:owid}.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{../Notebooks/USACasesPerDayRawNoLegendWave1Peak.png}
  \caption{USA COVID-19 cases per day from each daily report, 
    from the OWID COVID-19 data github repository.  Newer reports
    never restate older totals.}
  \label{fig:owid}
\end{figure}

I was surprised to find that there are no updates to past records!
How can it be that the total cases for a given day are completely
known the following day and never need to be updated?  Given what we
know about the NYC data, and given that the NYC data is a large part
of the USA data, it can't possibly be the case that on a given day
they know exactly the number of cases the day before.  Something else
must be going on.

So I entered an
\href{https://github.com/owid/covid-19-data/issues/41}{issue on the
  OWID COVID-19 github repository}.  I asked for clarification of how
they collect and publish the data.  Edouard Mathieu, Data Manager at
OWID, responded:
\begin{quotation}
  For confirmed cases and deaths, our data comes from the European
  Centre for Disease Prevention and Control (ECDC). We discuss how and
  when the ECDC collects and publishes this data \href{https://ourworldindata.org/coronavirus#our-world-in-data-relies-on-data-from-the-european-cdc}{here}.

  Importantly, the ECDC follows a general rule of not changing past
  values in its data. If cases/deaths are reported with a lag -- a
  general   lag, as you described, or occasional
  \href{https://www.theguardian.com/us-news/2020/apr/15/new-york-city-coronavirus-death-toll-jumps-revised-count}{'blocks'
    of new data}--these new
  cases/deaths will be added \bf{on the date that the country reported them
    to the ECDC.} \cite{Mathieu2020Dates}
\end{quotation}

So, OWID gets their data from the ECDC -- The European Center for
Disease Prevention and Control, and the ECDC doesn't collect data by
incident date, it collects the data by the date on which it receives
the reports.

Investigating other data hubs, I discovered that it's not just the
ECDC.  The \href{https://github.com/CSSEGISandData/COVID-19}{Johns
  Hopkins University COVID-19 repository}, and the
\href{https://github.com/nytimes/covid-19-data}{New York Times
  COVID-19 repository} also record instances by the report receipt
date instead of by the incident date.  So these databases, the major
sources of data that people use for modeling, for planning disease
responses, and for reporting, are collecting the data by reporting
date instead of by incident date.\nocite{JHU2020data,NYT2020data}

I followed up with Edouard Mathieu.  I asked him if he knew of any
rationale for why the data was being collected this way.  His
impression was that governments try to give the most accurate view and
record data based on incident date, updating history as needed.  On
the other hand, aggregators like WHO, ECDC and JHU are more concerned
with ease of aggregation and stability of reported numbers, so they
instead record data based on the reporting date. \cite{Mathieu2020Dates}

%% \begin{quotation}
%%   Hi Harvey,

%% Thanks for getting in touch. There's no perfectly clear and obvious
%% answer but here's how I understand it: it comes down to who is
%% publishing that data and what they consider their "role" to be.

%% - If they're a government, ministry, public health authority,
%% etc. whose job it is to report on the situation, then they try to give
%% the most accurate picture of how the epidemic evolved in the country,
%% including by going "back in time" and correcting past figures to make
%% them as close as possible to reality.

%% - On the other hand, organizations like the WHO, ECDC, JHU, consider
%% themselves to be data providers first and foremost. This means they
%% aim for "stability" in their data, and they avoid as much as possible
%% (or even completely) going back and changing past days, as the many
%% people/applications/dashboards/etc. relying on their data wouldn't
%% necessarily notice these changes and handle them correctly. 

%% Another issue is that while it's easy for 1 country to fix past
%% figures in a time series on its own website, it would be much harder
%% for an international organization that receives data from 200+
%% countries to accept retroactive changesâ€”their job is made a lot easier
%% by simply telling countries to send data corrections as if they were
%% big "blocks" of cases or deaths that suddenly afppear on a given day.

%% This is obviously an issue sometimes, especially when some of those
%% corrections are very large (for example New York City or China in
%% April), but we don't know of any large and reliable data provider that
%% reports in the way you're looking for.

%% Best,

%% Edouard
%% \end{quotation}

I also contacted Lauren Gardner, Associate Professor, Department of
Civil and Systems Engineering, Co-Director of Center for Systems
Science and Engineering (CSSE), Johns Hopkins University.  Professor
Gardner and her team are responsible for the
\href{https://coronavirus.jhu.edu/map.html}{Johns Hopkins COVID-19
  Dashboard}.  She agreed that there are issues with using reporting
dates rather than incident dates, but unfortunately, that's often all
that's available. \cite{Gardner2020Dates}

%% \begin{quotation}
%%   And yes, there are absolutely issues with using reporting data
%%   rather than incidence rate, however more often than not, that is all
%%   that is available.
%% \end{quotation}

\section{Garbage out?}

What's the big deal?  Counting by reporting date instead of incident
essentially takes some percentage of the actual data and moves it
later in time.  One would expect this to flatten and shift the curve.
It would make the infection rate appear lower before the
peak, make the peak appear later, and make the infection rate appear
to drop off more slowly after the peak.  Moreover, since sites will
report a number of days together, it also makes the data jumpier and
thus harder to analyze.

The problem is that scientists are using these numbers to model the
disease, the government is using these numbers to plan how to address
the risks, and the media is reporting about the numbers.  So it
potentially reduces the accuracy of the models, interferes with
planning and leads to hysterical media reports about fictitious rising
and falling of death counts.

How big is the effect, really?  To start, I calculated some simulated
results.  I consider the true counts to follow a normal distribution,
and compare them to what happens if 30\% or 50\% of the data comes in
late (Figure~\ref{fig:sim}).  It can have a substantial impact.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{../Notebooks/simulatedDelays.png}
  \caption{Impact of data being delayed.  If some of the data is
    reported late, then it is likely to understate the rise and
    delay the apperance of the fall.}
  \label{fig:sim}
\end{figure}

We can go further.  The NYC data has both the reporting dates
and the incident dates, so I backed out what it would look like if it
was recorded by report date instead of by incident
date. Figures~\ref{fig:peak1} and \ref{fig:omicron} compare the
report date results to the incident date results for the first peak
and for the Omicron peak.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{../Notebooks/casesPerDayHistoryRptDtVsInDtWave1Peak.png}
  \caption{NYC cases per day, first peak, by reporting date vs by
    incident date.  The reporting date data is very noisy and plagued
    by spurious peaks and drops, even after being averaged on a 7 day
    basis.}
  \label{fig:peak1}
\end{figure}

\begin{figure}[!htb]
  \centering
  \includegraphics[width=\textwidth]{../Notebooks/casesPerDayHistoryRptDtVsInDtWave4Peak.png}
  \caption{NYC cases per day, Omicron peak, by reporting date vs by
    incident date.  The reporting date data is as poorly behaved and
    misleading as for the first peak.}
  \label{fig:omicron}
\end{figure}

As you can see, the reporting date data is far noisier; so much so
that the 7 day cycle I documented in
\href{https://hjstein.blogspot.com/2020/04/covid-19-nyc-stats-ray-of-hope.html}{Covid-19
  NYC Stats - A Ray of Hope} is obscured and the 7 day rolling window
still shows substantial noise.  For example, the report date based
data exhibits a spike on May 11th of about 5,000 cases, far higher than
the incident based data shows.  The incident based data shows a slight
restatement of the data going back to the third week of March.

Presumably NYC received a report around mid May from a particular
site, and that report relayed daily infections back through March that
hadn't yet been recorded.  Report date based data then records this as
a huge spike which never actually occurred.  Because of these
underlying data collection mechanisms, report date based data
collection often give a substantially distorted view from day to day.

Another case in point is that the noise yields a false peak prior to
the actual peak.

Even after smoothing with a 7 day rolling window, the report date
data is overstating the post-peak number of cases by a substantial
amount, sometimes by over 50\%.

Surprisingly, the growth rate to the peak is higher rather than lower.
This is presumably because reporting delays were greater when the data
started being collected, leading to batches of reports coming in
together at a later date.

The Omicron peak shows similar results.  The counts by reporting date
show exaggerated spikes that are two to six times larger than the
actual counts for those dates, and the 7 day rolling average is noisy,
underestimates infection rates before the peak, and overestimates them
after the peak.

\section{Conclusions}
It is great that during the COVID-19 pandemic, organizations like OWID, the
WHO, and the ECDC, major news outlets like The New York Times, and
major universities, like Johns Hopkins University, are all collecting
and aggregating data on COVID-19 cases and deaths and making this data
publicly available.  Without such aggregation, it would be very
difficult to globally understand, analyze, and respond appropriately
to the pandemic.

On the other hand, it's unfortunate that they collect the data in a
way that obscures the current state of the disease and makes analysis
more difficult than it need be.  It causes the data to be
unnecessarily noisy, creates false peaks, understates rates leading up
to peaks, and overstates them afterwards.  It's disturbing that news
agencies are reporting on these numbers as if they actually occurred
on the reporting date, and governments are taking action based on
the same misconception.

One might be surprised that epidemiologists who make a career out of
analyzing epidemics and pandemics would record the data in such a
fashion.  But, such work tends to be on a longer time scale and it's
only in the current pandemic that we needed accurate, real-time
data.

I hope that data collection agencies will take it upon themselves to
correct this and begin collecting and aggregating the data on an
incident date basis instead of a reporting date basis.  This would be
a significant undertaking, but would better enable us to analyze the
data and respond appropriately, if not to this pandemic, then to the
next one.  Even better would be to release the data from the raw
reports.  Then no guesswork would be required to account for reporting
delays and analyses could be greatly enriched.

\printbibliography

\end{document}
